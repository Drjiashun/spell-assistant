# LASSO Feature Selection
# L1 Regularization
# L1-Regularization
# LASSO Feature Selection For classification
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import StandardScaler
np.random.seed(42)
num_samples = 500
spectrum_length = 200
n_classes = 3
X = np.random.randn(num_samples, spectrum_length)
y = np.random.randint(0, n_classes, num_samples)
def lasso_feature_selection(
        X: np.ndarray,
        y: np.ndarray,
        C: float = 0.1,
        min_features: int = 20,
        max_features: int = None,
        random_state: int = 42
):
    """
    Perform feature selection using LASSO (L1-regularized Logistic Regression) for classification tasks.
    """
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    current_C = C
    C_increase_factor = 2.0  # Increase C by this factor if too few features are selected
    max_attempts = 10  # Maximum number of iterations to tune C

    for attempt in range(max_attempts):
        try:
            estimator = LogisticRegression(
                penalty='l1',
                C=current_C,
                solver='liblinear',
                random_state=random_state,
                max_iter=5000
            )
            estimator.fit(X_scaled, y)
            selector = SelectFromModel(
                estimator=estimator,
                max_features=max_features,
                prefit=True
            )
            selected_indices = selector.get_support(indices=True)

            # Check if the number of selected features meets the minimum requirement
            if len(selected_indices) >= min_features:
                selected_features = X[:, selected_indices]
                coefficients = estimator.coef_.ravel() if estimator.coef_.ndim > 1 else estimator.coef_
                return selected_indices, coefficients
            else:
                # Increase C to reduce regularization and select more features
                current_C *= C_increase_factor
        except Exception as e:
            return np.array([]), np.zeros(X.shape[1])

    # If minimum features are not achieved, return the last attempt's results
    selected_features = X[:, selected_indices] if len(selected_indices) > 0 else np.array([])
    coefficients = estimator.coef_.ravel() if len(
        selected_indices) > 0 and estimator.coef_.ndim > 1 else estimator.coef_ if len(
        selected_indices) > 0 else np.zeros(X.shape[1])
    return selected_indices, coefficients
# Perform LASSO feature selection
C=0.1
min_features = 20
selected_indices, coefficients = lasso_feature_selection(
        X, y,
        C=C,
        min_features = min_features,
        max_features=None,
        random_state=42
)
print("--- LASSO Feature Selection Results ---")
print(f"Original data shape: {X.shape}")
print(f"Selected features shape: {selected_features.shape}")
print(f"Selected feature indices: {selected_indices.tolist()}")
if len(selected_indices) == 0:
    print("Warning: No features were selected. Consider increasing C or max_features.")