# CARS feature selection
# CARS
# Competitive Adaptive Reweighted Sampling feature selection for regression
# Competitive Adaptive Reweighted Sampling
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import PLSRegression
from sklearn.model_selection import KFold, cross_val_predict
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import warnings
np.random.seed(42)
num_samples = 1000
spectrum_length = 300
X = np.random.randn(num_samples, spectrum_length)
y = np.random.uniform(0, 10, num_samples)
print(f"Input data shape (X): {X.shape}")
print(f"Target data shape (y): {y.shape}")
scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)
scaler_y = StandardScaler() # Scale y for PLS internal modeling
y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()
print("Data standardized.")
def cars_pls(X, y, n_sampling_runs=50, n_components_pls=5, cv_folds=5, min_features=20, verbose=False):
    """
    Performs Competitive Adaptive Reweighted Sampling (CARS) with PLS for regression task.
    """
    n_samples, n_features = X.shape
    print(f"Starting CARS-PLS with {n_sampling_runs} sampling runs...")
    # --- Initialization ---
    pls_full = PLSRegression(n_components=min(n_components_pls, n_features, n_samples - cv_folds - 1))
    pls_full.fit(X, y)
    initial_coeffs = np.abs(pls_full.coef_.flatten())
    indices = np.arange(n_features)  # Track available indices
    selected_indices_history = []    # Store selected indices at each run
    rmse_cv_history = []             # Store RMSECV for each subset size
    coeffs_history = []              # Store coefficients/weights used for selection

    # --- Main CARS Loop ---
    for run in range(n_sampling_runs):
        sample_subset_indices = np.random.choice(n_samples, size=int(n_samples * 0.8), replace=True)
        X_subset = X[sample_subset_indices][:, indices]  # Select current features
        y_subset = y[sample_subset_indices]
        n_comp_subset = min(n_components_pls, X_subset.shape[1], X_subset.shape[0] - cv_folds - 1)
        if n_comp_subset < 1:
            warnings.warn(f"Run {run+1}: Not enough samples/features for PLS subset model. Using previous weights.")
            if run > 0:  # Use weights from before this failed run
                weights_in_run = [coeffs_history[-1]]
            else:  # Fails on first run - highly unlikely but possible
                weights_in_run = [np.abs(np.random.randn(len(indices)))]  # Random if first fails
        else:
            pls_subset = PLSRegression(n_components=n_comp_subset)
            try:
                pls_subset.fit(X_subset, y_subset)
                weights_in_run = [np.abs(pls_subset.coef_.flatten())]
            except Exception as e:
                warnings.warn(f"Run {run+1}: PLS subset fitting failed: {e}. Using previous weights.")
                if run > 0:
                    weights_in_run = [coeffs_history[-1]]
                else:
                    weights_in_run = [np.abs(np.random.randn(len(indices)))]  # Random if first fails
        coeffs_history.append(weights_in_run[0])  # Assuming weights_in_run always has one element
        ratio = (n_features/2)**(1/(n_sampling_runs-1)) * np.exp(-(np.log(n_features/2)/(n_sampling_runs-1))*run)  # EDF formula
        n_vars_to_keep = max(2, int(np.round(n_features * ratio)))  # Keep at least 2 variables
        current_variable_weights = weights_in_run[0]  # Use the weights calculated in this run
        sorted_weight_indices = np.argsort(current_variable_weights)[::-1]  # Indices sorted by weight desc
        selected_indices_in_current = sorted_weight_indices[:n_vars_to_keep]
        indices = indices[selected_indices_in_current]  # Update the pool of available indices
        X_eval = X[:, indices]  # Use selected features on FULL dataset
        n_comp_eval = min(n_components_pls, X_eval.shape[1], n_samples - cv_folds - 1)
        if n_comp_eval < 1:
            warnings.warn(f"Run {run+1}: Not enough features ({len(indices)}) for evaluation PLS. Assigning high RMSECV.")
            rmsecv = np.inf
        else:
            pls_eval = PLSRegression(n_components=n_comp_eval)
            cv = KFold(n_splits=cv_folds, shuffle=True, random_state=run)  # Use run as seed for CV split
            y_cv_pred = cross_val_predict(pls_eval, X_eval, y, cv=cv)
            rmsecv = np.sqrt(mean_squared_error(y, y_cv_pred))
        selected_indices_history.append(indices.copy())  # Store the selected indices
        rmse_cv_history.append(rmsecv)

        if verbose and (run + 1) % (n_sampling_runs // 10 + 1) == 0:
            print(f"  Run {run + 1}/{n_sampling_runs}: Keeping {len(indices)} vars, RMSECV={rmsecv:.4f}")
        n_features = len(indices)
        if n_features <= 1:  # Stop if only 1 or 0 features left
            print(f"Stopping early at run {run+1}: {n_features} variables remaining.")
            break

    # --- Select Optimal Subset with Minimum Features Constraint ---
    valid_indices = [i for i, indices in enumerate(selected_indices_history) if len(indices) >= min_features]
    if not valid_indices:
        warnings.warn(f"No subsets with at least {min_features} features found. Returning subset with most features.")
        valid_indices = [np.argmax([len(indices) for indices in selected_indices_history])]
    min_rmsecv_index = valid_indices[np.argmin([rmse_cv_history[i] for i in valid_indices])]
    selected_indices = selected_indices_history[min_rmsecv_index]
    min_rmsecv = rmse_cv_history[min_rmsecv_index]
    print(f"Optimal number of variables: {len(selected_indices)} found at run {min_rmsecv_index + 1}")
    print(f"Minimum RMSECV: {min_rmsecv:.4f}")
    history = {
        'n_selected_vars': [len(s) for s in selected_indices_history],
        'rmsecv': rmse_cv_history,
        'selected_indices_history': selected_indices_history,
    }
    selected_features = X[:, selected_indices] if len(selected_indices) > 0 else np.array([[]])
    return selected_indices, history
N_CARS_RUNS = 50
N_PLS_COMP = 10  # PLS components for internal models
CV_FOLDS_CARS = 5
MIN_FEATURES = 20
selected_indices, cars_history = cars_pls(
    X_scaled, y_scaled, # Use scaled data
    n_sampling_runs=N_CARS_RUNS,
    n_components_pls=N_PLS_COMP,
    cv_folds=CV_FOLDS_CARS,
    min_features=MIN_FEATURES,
    verbose=True
)
print("\n--- CARS Results ---")
print(f"Number of selected features: {len(selected_features_cars)}")
print(f"Selected feature indices (CARS): {selected_features_cars}")
plt.figure(figsize=(10, 5))
plt.plot(range(1, len(cars_history['n_selected_vars']) + 1), cars_history['n_selected_vars'], '-o', markersize=4)
plt.xlabel('Sampling Run')
plt.ylabel('Number of Selected Variables')
plt.title('CARS: Variable Selection Progress')
plt.grid(True, linestyle=':')
plt.show()
min_rmse_idx = np.argmin(cars_history['rmsecv'])
plt.figure(figsize=(10, 5))
plt.plot(range(1, len(cars_history['rmsecv']) + 1), cars_history['rmsecv'], '-o', markersize=4)
plt.xlabel('Sampling Run')
plt.ylabel('RMSECV')
plt.title('CARS: RMSECV vs. Sampling Run')
plt.scatter(min_rmse_idx + 1, cars_history['rmsecv'][min_rmse_idx], marker='*', s=150, c='red', label=f'Optimal Run ({min_rmse_idx+1})')
plt.legend()
plt.grid(True, linestyle=':')
plt.show()
plt.figure(figsize=(12, 6))
plt.stem(np.arange(spectrum_length), np.isin(np.arange(spectrum_length), selected_features_cars).astype(int),
         linefmt='g-', markerfmt='go', basefmt=' ', label='CARS Selected Features')
plt.xlabel('Feature Index'); plt.ylabel('Selected (1) / Not Selected (0)')
plt.title(f'CARS Selected Variables (K={len(selected_features_cars)})')
plt.yticks([0, 1]); plt.ylim(-0.2, 1.1); plt.legend(); plt.grid(True, linestyle=':')
plt.show()
