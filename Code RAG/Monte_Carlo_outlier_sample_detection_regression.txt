# Monte Carlo outlier
# Monte Carlo outlier detection
# Monte Carlo outlier samples detection
# Monte Carlo outlier sample detection
# Monte Carlo outlier sample detection for regression tasks
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import ShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.cross_decomposition import PLSRegression
import pandas as pd
from tqdm import tqdm
np.random.seed(42)
num_samples = 1000
spectrum_length = 300
X = np.random.randn(num_samples, spectrum_length)
y = np.random.uniform(0, 10, num_samples)
def detect_outliers_mccv(X, y, n_splits=100, test_size=0.2, n_pls_components=5, error_threshold_std_factor=3.0):
    """
    Detect outliers using Monte Carlo Cross-Validation (MCCV) based on prediction errors for regression tasks.
    """
    if isinstance(X, pd.DataFrame):
        X_values = X.values
        original_indices = X.index
    else:
        X_values = np.asarray(X)
        original_indices = np.arange(X_values.shape[0])
    y_values = np.asarray(y)
    n_samples = X_values.shape[0]
    sample_sum_sq_errors = np.zeros(n_samples)
    sample_validation_counts = np.zeros(n_samples, dtype=int)
    mccv = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=42)
    print(f"Running MCCV with {n_splits} splits...")
    for train_idx, test_idx in tqdm(mccv.split(X_values), total=n_splits):
        X_train, X_test = X_values[train_idx], X_values[test_idx]
        y_train, y_test = y_values[train_idx], y_values[test_idx]
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)  # Use scaler fitted on train data
        pls = PLSRegression(n_components=n_pls_components)
        try:
            pls.fit(X_train_scaled, y_train)
        except ValueError as e:
            print(
                f"Warning: Could not fit PLS model in one split (likely too few samples or components). Skipping split. Error: {e}")
            continue  # Skip this split if PLS fails
        y_pred = pls.predict(X_test_scaled).ravel()  # Flatten prediction array
        squared_errors = (y_test - y_pred) ** 2
        np.add.at(sample_sum_sq_errors, test_idx, squared_errors)
        np.add.at(sample_validation_counts, test_idx, 1)
    valid_counts_mask = sample_validation_counts > 0
    sample_avg_mse = np.full(n_samples, np.nan)  # Initialize with NaN
    sample_avg_mse[valid_counts_mask] = sample_sum_sq_errors[valid_counts_mask] / sample_validation_counts[
        valid_counts_mask]
    sample_avg_rmse = np.sqrt(sample_avg_mse)
    if np.any(~valid_counts_mask):
        print(f"Warning: {np.sum(~valid_counts_mask)} samples were never included in the validation set.")
    valid_rmse = sample_avg_rmse[valid_counts_mask]
    if len(valid_rmse) == 0:
        print("Warning: No samples were validated. Cannot determine outliers.")
        return np.array([], dtype=int), sample_avg_rmse
    mean_rmse = np.nanmean(sample_avg_rmse)  # Use nanmean to ignore potential NaNs
    std_rmse = np.nanstd(sample_avg_rmse)  # Use nanstd
    error_threshold = mean_rmse + error_threshold_std_factor * std_rmse
    print(f"\nAverage RMSE across samples: {mean_rmse:.4f}")
    print(f"Std Dev of sample RMSEs: {std_rmse:.4f}")
    print(f"Outlier threshold (Mean + {error_threshold_std_factor}*StdDev): {error_threshold:.4f}")
    outlier_mask = sample_avg_rmse > error_threshold
    outlier_indices_local = np.where(outlier_mask)[0]
    outlier_indices = original_indices[outlier_indices_local]
    print(f"Number of potential outliers detected: {len(outlier_indices)}")
    return outlier_indices, sample_avg_rmse

optimal_pls_components = 4  # Assume we determined this beforehand
n_splits=200
error_threshold_std_factor=3.0
outlier_idxs, sample_rmse = detect_outliers_mccv(
        X,
        y,
        n_splits=n_splits,  # More splits give more stable error estimates
        test_size=0.25,  # Size of validation set in each split
        n_pls_components=optimal_pls_components,
        error_threshold_std_factor=error_threshold_std_factor  # Threshold: Mean + 3 * StdDev
)
print(f"\nIndices of detected outliers:")
print(outlier_idxs)
# Create boolean mask from outlier indices
outlier_mask = np.zeros(X.shape[0], dtype=bool)
outlier_mask[outlier_idxs] = True
X_clean = X[~outlier_mask]
y_clean = y[~outlier_mask]
print(X_clean.shape,y_clean.shape)
plt.figure(figsize=(10, 6))
plt.hist(sample_rmse[~np.isnan(sample_rmse)], bins=30, color='lightblue', edgecolor='black',
             label='Sample Avg. RMSE Distribution')

# Highlight detected outliers
if len(outlier_idxs) > 0:
        # Find the positions of outlier_idxs in the original 0-based indexing
    if isinstance(X, pd.DataFrame):
        outlier_positions = X.index.get_indexer(outlier_idxs)
    else:
        outlier_positions = outlier_idxs  # Already 0-based if X is numpy array
    plt.scatter(sample_rmse[outlier_positions], np.zeros_like(outlier_positions) + 1, color='red', marker='x', s=80,
                    label='Detected Outliers')

# Plot threshold line
mean_rmse_val = np.nanmean(sample_rmse)
std_rmse_val = np.nanstd(sample_rmse)
threshold = mean_rmse_val + 3.0 * std_rmse_val
plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold (Mean + 3*Std)')
plt.xlabel("Average Sample RMSE across MCCV splits")
plt.ylabel("Frequency / Outlier Marker")
plt.title("Distribution of Sample Prediction Errors from MCCV")
plt.legend()
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.yticks([])  # Hide y-axis frequency ticks for clarity with markers
plt.tight_layout()
plt.show()