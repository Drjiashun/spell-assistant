# Backward Elimination
# Backward Elimination Feature Selection
# Backward Elimination Feature Selection For Regression
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
np.random.seed(42)
num_samples = 1000
spectrum_length = 300
X = np.random.randn(num_samples, spectrum_length)
y = np.random.uniform(0, 10, num_samples)
def backward_elimination(X, y, p_threshold=0.05):
    """
    Perform Backward Elimination for feature selection in regression tasks
    """
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    n_features = X_scaled.shape[1]
    selected_features = list(range(n_features))
    while len(selected_features) > 0:
        X_current = X_scaled[:, selected_features]
        model = LinearRegression()
        model.fit(X_current, y)
        y_pred = model.predict(X_current)
        residual = y - y_pred
        mse = np.mean(residual ** 2)
        n = len(y)
        X_with_const = np.column_stack([np.ones(n), X_current])
        XtX_inv = np.linalg.inv(np.dot(X_with_const.T, X_with_const))
        se = np.sqrt(mse * np.diag(XtX_inv)[1:])  # Exclude intercept
        t_stats = model.coef_ / se
        from scipy.stats import t
        p_values = 2 * (1 - t.cdf(np.abs(t_stats), df=n - len(selected_features) - 1))
        max_p_idx = np.argmax(p_values) if len(p_values) > 0 else -1
        max_p_value = p_values[max_p_idx] if max_p_idx != -1 else 0
        if max_p_value < p_threshold or len(selected_features) == 0:
            break
        selected_features.pop(max_p_idx)

    return selected_features
Threshold = 0.05
selected_features = backward_elimination(X, y, p_threshold=Threshold)
print(f"Selected feature indices: {selected_features}")
print(f"Number of selected features: {len(selected_features)}")