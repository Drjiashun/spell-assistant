# Backward Interval PLS
# BiPLS Feature Selection
# Backward Interval Partial Least Squares
# Backward Interval Partial Least Squares Feature Selection For Classification
import numpy as np
import math
from sklearn.cross_decomposition import PLSRegression
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler, LabelEncoder

np.random.seed(42)
num_samples = 500
spectrum_length = 200
n_classes = 3
X = np.random.randn(num_samples, spectrum_length)
y = np.random.randint(0, n_classes, num_samples)
def calculate_misclassification_rate(X: np.ndarray, y: np.ndarray, n_comp: int, cv_folds: int) -> float:
    if X.shape[1] == 0 or n_comp <= 0:
        return np.inf
    max_possible_comp = min(X.shape[1], X.shape[0] - math.ceil(X.shape[0] / cv_folds))
    if max_possible_comp <= 0:
        return np.inf
    actual_n_comp = min(n_comp, max_possible_comp)
    try:
        model = PLSRegression(n_components=actual_n_comp)
        cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
        y_pred = np.zeros_like(y, dtype=float)
        for train_idx, test_idx in cv.split(X):
            X_train, X_test = X[train_idx], X[test_idx]
            y_train = y[train_idx]
            model.fit(X_train, y_train)
            y_pred[test_idx] = model.predict(X_test).ravel()
        y_pred_class = (y_pred >= 0.5).astype(int) if len(np.unique(y)) == 2 else np.argmax(y_pred, axis=1)
        return 1.0 - accuracy_score(y, y_pred_class)
    except Exception as e:
        return np.inf
def bipls_feature_selection_classification(
        X: np.ndarray,
        y: np.ndarray,
        n_intervals: int = 20,
        max_pls_components: int = 5,
        cv_folds: int = 5,
        min_intervals_to_keep: int = 2,
        misclassification_increase_threshold: float = 1.05
):
    """
    Backward Interval PLS for feature selection in classification tasks.
    """
    scaler_X = StandardScaler()
    X_scaled = scaler_X.fit_transform(X)
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    # For binary classification, use 0/1 encoding; for multi-class, use one-hot encoding
    if len(le.classes_) == 2:
        y_pls = y_encoded.astype(float)
    else:
        y_pls = np.zeros((y_encoded.shape[0], len(le.classes_)))
        y_pls[np.arange(y_encoded.shape[0]), y_encoded] = 1
    n_features = X.shape[1]
    interval_size = math.ceil(n_features / n_intervals)
    intervals = [
        list(range(i * interval_size, min((i + 1) * interval_size, n_features)))
        for i in range(n_intervals) if i * interval_size < n_features
    ]
    active_intervals = intervals.copy()
    active_features = list(range(n_features))
    misclassification_history = []
    removed_intervals = []
    current_misclassification = calculate_misclassification_rate(
        X_scaled[:, active_features], y_pls, max_pls_components, cv_folds
    )
    misclassification_history.append(current_misclassification)
    print(f"Initial Misclassification Rate: {current_misclassification:.4f}, "
          f"Features: {len(active_features)}, Intervals: {len(active_intervals)}")
    iteration = 0
    while len(active_intervals) > min_intervals_to_keep:
        iteration += 1
        misclassification_scores = []
        feature_sets = []
        for i, interval in enumerate(active_intervals):
            temp_features = [idx for idx in active_features if idx not in interval]
            misclassification = calculate_misclassification_rate(
                X_scaled[:, temp_features], y_pls, max_pls_components, cv_folds
            )
            misclassification_scores.append(misclassification)
            feature_sets.append(temp_features)
        best_idx = np.argmin(misclassification_scores)
        best_misclassification = misclassification_scores[best_idx]
        if best_misclassification > current_misclassification * misclassification_increase_threshold:
            print(f"Stopping: Misclassification rate increased beyond threshold "
                  f"({best_misclassification:.4f} > {current_misclassification * misclassification_increase_threshold:.4f})")
            break
        removed_interval = active_intervals.pop(best_idx)
        active_features = feature_sets[best_idx]
        current_misclassification = best_misclassification
        misclassification_history.append(current_misclassification)
        removed_intervals.append(f"{removed_interval[0]}-{removed_interval[-1]}")
        if not active_features:
            print("Stopping: No features remaining")
            break
    return np.array(sorted(active_features)), misclassification_history, removed_intervals

n_intervals = 15
max_pls_components=5
cv_folds=5
min_intervals_to_keep=2
selected_features, misclassification_history, removed_intervals = bipls_feature_selection_classification(
        X, y,
        n_intervals=n_intervals,
        max_pls_components=max_pls_components,
        cv_folds=cv_folds,
        min_intervals_to_keep=min_intervals_to_keep
    )
print("\n--- BiPLS Classification Results ---")
print(f"Selected features: {len(selected_features)} features at indices {selected_features}")
print(f"Final data shape: ({X.shape[0]}, {len(selected_features)})")
min_misclassification_idx = np.argmin(misclassification_history)
print(f"\nBest Misclassification Rate: {misclassification_history[min_misclassification_idx]:.4f} "
          f"at iteration {min_misclassification_idx}")